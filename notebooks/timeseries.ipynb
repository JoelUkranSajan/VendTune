{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ce3d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import geopandas as gpd\n",
    "import os\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import MultiPolygon, Polygon\n",
    "import time\n",
    "import psycopg2\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely import wkb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cec6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_postgres(query, postgres_host, postgres_port, database_name, db_user, db_password):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    conn = None\n",
    "    try:\n",
    "        # Connect to PostgreSQL\n",
    "        conn = psycopg2.connect(\n",
    "            database=database_name,\n",
    "            user=db_user,\n",
    "            password=db_password,\n",
    "            host=postgres_host,\n",
    "            port=postgres_port\n",
    "        )\n",
    "        print(\"PostgreSQL connection opened\")\n",
    "        \n",
    "        # Create a cursor object\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Execute the query\n",
    "        cursor.execute(query)\n",
    "        print(\"Query executed successfully\")\n",
    "        \n",
    "        # Fetch all rows from the executed query\n",
    "        rows = cursor.fetchall()\n",
    "        \n",
    "        # Get column names from the cursor\n",
    "        colnames = [desc[0] for desc in cursor.description]\n",
    "        \n",
    "        # Convert the result to a pandas DataFrame\n",
    "        df = pd.DataFrame(rows, columns=colnames)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return None\n",
    "\n",
    "    finally:\n",
    "        # Closing the connection\n",
    "        if conn:\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            print(\"PostgreSQL connection closed.\")\n",
    "\n",
    "    print(f'connect_to_postgres took {time.time() - start_time} seconds to complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f206101",
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_host = 'localhost'\n",
    "postgres_port = 5432 \n",
    "database_name = 'busyness'\n",
    "db_user = 'crafty'\n",
    "db_password = 'winner'\n",
    "table_name = 'public.busyness_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074a52b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "    SELECT\n",
    "    DATE_TRUNC('minute', TO_TIMESTAMP(timestamp)) AS minute, SUM(busynessscore) AS total_busyness\n",
    "    FROM\n",
    "        busyness_data\n",
    "    WHERE \n",
    "        zonenumber = 70 and mode != 'subway'\n",
    "    GROUP BY\n",
    "        DATE_TRUNC('minute', TO_TIMESTAMP(timestamp))\n",
    "    ORDER BY\n",
    "        minute; \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3043bf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "    SELECT\n",
    "    DATE_TRUNC('hour', TIMESTAMP WITH TIME ZONE 'epoch' + timestamp * INTERVAL '1 SECOND') +\n",
    "        ((EXTRACT(MINUTE FROM TIMESTAMP WITH TIME ZONE 'epoch' + timestamp * INTERVAL '1 SECOND')::int / 10) * 10 || ' minutes')::interval AS ten_minute_interval,\n",
    "    SUM(busynessscore) AS total_busyness\n",
    "FROM\n",
    "    busyness_data\n",
    "WHERE \n",
    "    zonenumber = 70 AND mode != 'subway'\n",
    "GROUP BY\n",
    "    DATE_TRUNC('hour', TIMESTAMP WITH TIME ZONE 'epoch' + timestamp * INTERVAL '1 SECOND') +\n",
    "        ((EXTRACT(MINUTE FROM TIMESTAMP WITH TIME ZONE 'epoch' + timestamp * INTERVAL '1 SECOND')::int / 10) * 10 || ' minutes')::interval\n",
    "ORDER BY\n",
    "    ten_minute_interval; \n",
    "    '''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9534546",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = connect_to_postgres(query, postgres_host, postgres_port, database_name, db_user, db_password)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f394e0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.index[:3])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fcae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['unix_timestamp'] = df['minute'].apply(lambda x: x.timestamp()/10**9)\n",
    "\n",
    "# Plotting the data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df['unix_timestamp'], df['total_busyness'], linestyle='-')\n",
    "plt.title('Busyness Score Over Time')\n",
    "plt.xlabel('Unix Timestamp')\n",
    "plt.ylabel('Busyness Score')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "# plt.xlim(1.6444,1.9)\n",
    "plt.xlim(1.696*10**9,1.7*10**9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1410770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['unix_timestamp'] = df['minute'].apply(lambda x: x.timestamp()/10**9)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df['ten_minute_interval'], df['cumulative_busyness'], linestyle='-')\n",
    "plt.title('Busyness Score Over Time')\n",
    "plt.xlabel('ten_minute_interval')\n",
    "plt.ylabel('Busyness Score')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.xlim(1.67,1.68)\n",
    "# plt.xlim(1.68475,1.6850)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ec6430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['ten_minute_interval'] = pd.to_datetime(df['minute'])\n",
    "df['ten_minute_interval'] = pd.to_datetime(df['ten_minute_interval'], utc=True)\n",
    "df = df.sort_values(by='ten_minute_interval')\n",
    "\n",
    "# Extract date from 'minute' column for grouping\n",
    "df['date'] = df['ten_minute_interval'].dt.date\n",
    "\n",
    "# Compute cumulative sum of 'total_busyness' within each day\n",
    "df['cumulative_busyness'] = df.groupby('date')['total_busyness'].cumsum()\n",
    "\n",
    "# Drop the 'date' column as it's no longer needed\n",
    "df = df.drop(columns=['date'])\n",
    "\n",
    "\n",
    "df['unix_timestamp'] = df['ten_minute_interval'].apply(lambda x: time.mktime(x.timetuple()))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04090ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort DataFrame by 'minute' (if not already sorted)\n",
    "df = df.sort_values(by='minute')\n",
    "\n",
    "# Split into training and testing sets\n",
    "train_size = int(len(df) * 0.1)  # 80% for training\n",
    "train_data = df.iloc[:train_size]\n",
    "test_data = df.iloc[:train_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bff3e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d847be09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Fit ARIMA model\n",
    "model = ARIMA(train_data['cumulative_busyness'][23000:], order=(2, 0, 0))  # Example: AR(1) model\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Print model summary (optional)\n",
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62bdcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "predictions = model_fit.forecast(len(test_data))\n",
    "\n",
    "# Evaluate predictions against actual values\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(test_data['cumulative_busyness'], predictions)\n",
    "print(f'Mean Squared Error (MSE): {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15c8890",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = model_fit.forecast(steps=10)\n",
    "print(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610c0e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_fit.forecast(len(test_data))  # Example: Replace with your predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b3ee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_fit.forecast(len(test_data))  # Example: Replace with your predictions\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting actual data\n",
    "plt.figure(figsize=(14, 7))\n",
    "# plt.plot(test_data['minute'][10:], test_data['cumulative_busyness'][10:], label='Actual')\n",
    "\n",
    "# Plotting predictions\n",
    "plt.plot(test_data['minute'][10:], predictions[10:], label='Predicted')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Total Busyness')\n",
    "plt.title('Actual vs Predicted Total Busyness')\n",
    "plt.legend()\n",
    "\n",
    "# Display plot\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da22e3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "plt.plot(train_data['minute'][10:], train_data['cumulative_busyness'][10:])\n",
    "# plt.xlim(\"2002-10-25 17:20:00+00:00\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78444f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4110516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(train_data.index[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6f0b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('test_business_score_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e7bfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['minute'] = pd.to_datetime(train_data['minute'])\n",
    "\n",
    "# Add Unix timestamp column\n",
    "train_data['unix_timestamp'] = train_data['minute'].apply(lambda x: time.mktime(x.timetuple()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cc5a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data\n",
    "start_day = 1648480740.0-432000\n",
    "end_day = 1648480740.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22a4829",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_timestamp = train_data['unix_timestamp'].max()\n",
    "print(max_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d7c5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_day = 1.696*10**9\n",
    "end_day = 1.7*10**9\n",
    "\n",
    "# Create subset DataFrame with Unix timestamps between start_day and end_day\n",
    "subset_df = df[(df['unix_timestamp'] >= start_day) & (df['unix_timestamp'] <= end_day)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9112ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bafc248",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(y),max(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da63281",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = subset_df['unix_timestamp']\n",
    "y = subset_df['total_busyness']\n",
    "\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8556b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fa9d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Fit ARIMA model\n",
    "model = ARIMA(subset_df['cumulative_busyness'], order=(5, 1, 10))  # Example: AR(1) model\n",
    "model_fit = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dfed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_fit.forecast(len(subset_df))  # Example: Replace with your predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8244c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting actual data\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(subset_df['unix_timestamp'], subset_df['cumulative_busyness'], label='Actual')\n",
    "\n",
    "# Plotting predictions\n",
    "plt.plot(subset_df['unix_timestamp'], predictions, label='Actual')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Total Busyness')\n",
    "plt.title('Actual vs Predicted Total Busyness')\n",
    "plt.legend()\n",
    "\n",
    "# Display plot\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e77850",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(test)):\n",
    "    model = ARIMA(history, order=(5,1,0))\n",
    "    model_fit = model.fit()\n",
    "    output = model_fit.forecast()\n",
    "    yhat = output[0]\n",
    "    predictions.append(yhat)\n",
    "    obs = test[t]\n",
    "    history.append(obs)\n",
    "    print('predicted=%f, expected=%f' % (yhat, obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef84654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np             #for numerical computations like log,exp,sqrt etc\n",
    "import pandas as pd            #for reading & storing data, pre-processing\n",
    "import matplotlib.pylab as plt #for visualization\n",
    "#for making sure matplotlib plots are generated in Jupyter notebook itself\n",
    "%matplotlib inline             \n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a7c626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset_df = subset_df.drop(columns = {'minute','total_busyness'})\n",
    "# new_order = ['unix_timestamp','cumulative_busyness']\n",
    "subset_df = subset_df[new_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953e1f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.index[:3])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c406d15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine rolling statistics\n",
    "rolmean = subset_df['total_busyness'].rolling(window=24).mean() #window size 12 denotes 12 months, giving rolling mean at yearly level\n",
    "rolstd = subset_df['total_busyness'].rolling(window=24).std()\n",
    "print(rolmean,rolstd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e488d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = plt.plot(subset_df['unix_timestamp'],subset_df['total_busyness'], color='blue', label='Original')\n",
    "mean = plt.plot(subset_df['unix_timestamp'],rolmean, color='red', label='Rolling Mean')\n",
    "std = plt.plot(subset_df['unix_timestamp'],rolstd, color='black', label='Rolling Std')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Rolling Mean & Standard Deviation')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b21abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97a2563",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Results of Dickey Fuller Test:')\n",
    "dftest = adfuller(subset_df['total_busyness'], autolag='AIC')\n",
    "\n",
    "dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "for key,value in dftest[4].items():\n",
    "    dfoutput['Critical Value (%s)'%key] = value\n",
    "    \n",
    "dfoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8af271",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexedDataset_logScale = np.log(subset_df['total_busyness']-min(subset_df['total_busyness'])+1)\n",
    "print(subset_df['total_busyness']-min(subset_df['total_busyness'])+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58735ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexedDataset_logScale = np.log(subset_df['total_busyness']-min(subset_df['total_busyness'])+1)\n",
    "plt.plot(subset_df['unix_timestamp'],indexedDataset_logScale)#subset_df['total_busyness'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7535f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "movingAverage = indexedDataset_logScale.rolling(window=24).mean()\n",
    "movingSTD = indexedDataset_logScale.rolling(window=24).std()\n",
    "plt.plot(indexedDataset_logScale)\n",
    "plt.plot(movingAverage, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e93a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetLogScaleMinusMovingAverage = indexedDataset_logScale - movingAverage\n",
    "datasetLogScaleMinusMovingAverage.head(12)\n",
    "\n",
    "#Remove NAN values\n",
    "datasetLogScaleMinusMovingAverage.dropna(inplace=True)\n",
    "datasetLogScaleMinusMovingAverage.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8761be2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stationarity(timeseries,windows):\n",
    "    \n",
    "    #Determine rolling statistics\n",
    "    movingAverage = timeseries.rolling(window=windows).mean()\n",
    "    movingSTD = timeseries.rolling(window=windows).std()\n",
    "    \n",
    "    #Plot rolling statistics\n",
    "    orig = plt.plot(timeseries, color='blue', label='Original')\n",
    "    mean = plt.plot(movingAverage, color='red', label='Rolling Mean')\n",
    "    std = plt.plot(movingSTD, color='black', label='Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    #Perform Dickey–Fuller test:\n",
    "    print('Results of Dickey Fuller Test:')\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print(dfoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b284f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stationarity(datasetLogScaleMinusMovingAverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c143990",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetLogDiffShifting = indexedDataset_logScale - indexedDataset_logScale.shift()\n",
    "plt.plot(datasetLogDiffShifting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dc2dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AR Model\n",
    "#making order=(2,1,0) gives RSS=1.5023\n",
    "model = ARIMA(indexedDataset_logScale, order=(2,1,0))\n",
    "results_AR = model.fit()\n",
    "plt.plot(datasetLogDiffShifting)\n",
    "plt.plot(results_AR.fittedvalues, color='red')\n",
    "plt.title('RSS: %.4f'%sum((results_AR.fittedvalues - datasetLogDiffShifting)**2))\n",
    "print('Plotting AR model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87153b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94676e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a539f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(indexedDataset_logScale, order=(0,1,2))\n",
    "results_MA = model.fit()\n",
    "plt.plot(datasetLogDiffShifting)\n",
    "plt.plot(results_MA.fittedvalues, color='red')\n",
    "plt.title('RSS: %.4f'%sum((results_MA.fittedvalues - datasetLogDiffShifting)**2))\n",
    "print('Plotting MA model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e9f74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95094bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e82e59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263f1a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = connect_to_postgres(query, postgres_host, postgres_port, database_name, db_user, db_password)\n",
    "\n",
    "# df['unix_timestamp'] = df['ten_minute_interval'].apply(lambda x: x.timestamp()/10**9)\n",
    "# df = df.drop(columns = {'ten_minute_interval'})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dbe075",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pos_bus'] = df['cumulative_busyness']-min(df['cumulative_busyness'])+1\n",
    "df['pos_bus'] = df['pos_bus'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183f4436",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Rolling_5'] = df['total_busyness'].rolling(window=5).mean()\n",
    "df['Rolling_10'] = df['total_busyness'].rolling(window=10).mean()\n",
    "df['Rolling_20'] = df['total_busyness'].rolling(window=20).mean()\n",
    "df['Rolling_50'] = df['total_busyness'].rolling(window=50).mean()\n",
    "df['Rolling_100'] = df['total_busyness'].rolling(window=100).mean()\n",
    "df['Rolling_200'] = df['total_busyness'].rolling(window=200).mean()\n",
    "df['Rolling_500'] = df['total_busyness'].rolling(window=500).mean()\n",
    "df['Rolling_1000'] = df['total_busyness'].rolling(window=1000).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab249998",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cumulative_busyness'] = df['cumulative_busyness'].astype(float)\n",
    "df['ts'] = df['cumulative_busyness']-movingAverage\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3173fb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexedDataset_logScale = np.log(df['pos_bus'])\n",
    "movingAverage = df['cumulative_busyness'].rolling(window=24).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c985ab1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = df['hour']\n",
    "y =df['pos_bus']\n",
    "\n",
    "plt.plot(df['ts'][17000:17500])\n",
    "plt.xlim(17000,17500)\n",
    "plt.show()\n",
    "test_stationarity(df['ts'][17000:17500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfa1a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Fit ARIMA model\n",
    "model = ARIMA(df['ts'][17000:17500], order=(2, 0, 0))  # Example: AR(1) model\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Print model summary (optional)\n",
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e363dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_fit.forecast(len(df['ts'][17000:17500]))  # Example: Replace with your predictions\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting actual data\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(df['ts'][17000:17500], label='Actual')\n",
    "\n",
    "# Plotting predictions\n",
    "plt.plot(predictions, label='Predicted')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Total Busyness')\n",
    "plt.title('Actual vs Predicted Total Busyness')\n",
    "plt.legend()\n",
    "\n",
    "# Display plot\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1978c9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(df['ts'][17000:17500], order=(0,1,2))\n",
    "results_MA = model.fit()\n",
    "plt.plot(df['ts'][17000:17500],'bo')\n",
    "plt.plot(results_MA.fittedvalues, color='red')\n",
    "plt.title('RSS: %.4f'%sum((results_MA.fittedvalues - datasetLogDiffShifting)**2))\n",
    "print('Plotting MA model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04914e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "area: zone\n",
    "    \n",
    "time: prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7fb16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['unix_timestamp']\n",
    "\n",
    " \n",
    "\n",
    "y =df['Rolling_5'] \n",
    "plt.plot(x,y)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "y =df['Rolling_10'] \n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.show()\n",
    "\n",
    "y =df['Rolling_20'] \n",
    "plt.plot(x,y)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "y =df['Rolling_50'] \n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.show()\n",
    "\n",
    "y =df['Rolling_100']\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.show()\n",
    "\n",
    "y =df['Rolling_200']\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.show()\n",
    "\n",
    "y =df['Rolling_500']\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.show()\n",
    "\n",
    "y =df['Rolling_1000']\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.xlim(1.696,1.698)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea81307",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y)\n",
    "# plt.xlim(1.696,1.698)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d481990",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "WITH min_max_timestamps AS (\n",
    "    SELECT\n",
    "        MIN(TO_TIMESTAMP(timestamp)) AS min_timestamp,\n",
    "        MAX(TO_TIMESTAMP(timestamp)) AS max_timestamp\n",
    "    FROM\n",
    "        busyness_data\n",
    "    WHERE \n",
    "        zonenumber = 70\n",
    "        AND mode != 'subway'\n",
    "),\n",
    "\n",
    "hour_series AS (\n",
    "    SELECT generate_series(\n",
    "        DATE_TRUNC('hour', min_max_timestamps.min_timestamp),\n",
    "        DATE_TRUNC('hour', min_max_timestamps.max_timestamp),\n",
    "        '1 hour'::interval\n",
    "    ) AS hour\n",
    "    FROM min_max_timestamps\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    hour_series.hour AS hour,\n",
    "    COALESCE(SUM(busynessscore), 0) AS total_busyness\n",
    "FROM\n",
    "    hour_series\n",
    "LEFT JOIN (\n",
    "    SELECT\n",
    "        DATE_TRUNC('hour', TO_TIMESTAMP(timestamp)) AS hour,\n",
    "        SUM(busynessscore) AS busynessscore\n",
    "    FROM\n",
    "        busyness_data\n",
    "    WHERE \n",
    "        zonenumber = 70\n",
    "        AND mode != 'subway'\n",
    "    GROUP BY\n",
    "        DATE_TRUNC('hour', TO_TIMESTAMP(timestamp))\n",
    ") AS aggregated_data ON hour_series.hour = aggregated_data.hour\n",
    "GROUP BY\n",
    "    hour_series.hour\n",
    "ORDER BY\n",
    "    hour_series.hour;\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e115a08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939c5623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexedDataset_logScale = np.log(indexedDataset)\n",
    "# df['ts_pos'] = df['ts'] - min(df['ts'])\n",
    "df['logscale'] = np.log(df['ts_pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62ca180",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2f910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df['logscale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15374f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "min(df['ts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52423c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['logscale+'] = df['logscale+'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e845e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stationarity(df['logscale+'],240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81ce589",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolmean = df['logscale'].rolling(window=240).mean() #window size 12 denotes 12 months, giving rolling mean at yearly level\n",
    "rolstd = df['logscale'].rolling(window=240).std()\n",
    "print(rolmean,rolstd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe4bc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = plt.plot(df['logscale'], color='blue', label='Original')\n",
    "mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "std = plt.plot(rolstd, color='black', label='Rolling Std')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Rolling Mean & Standard Deviation')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a44d523",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['logscale+'] = df['logscale']-rolmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17188c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df['logscale+'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16a2bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(df['logscale+'], order=(2,1,2))\n",
    "results_MA = model.fit()\n",
    "plt.plot(df['logscale+'],'bo')\n",
    "plt.plot(results_MA.fittedvalues, color='red')\n",
    "plt.title('RSS: %.4f'%sum((results_MA.fittedvalues - datasetLogDiffShifting)**2))\n",
    "print('Plotting MA model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b09995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df['logscale+'],'bo')\n",
    "plt.plot(results_MA.fittedvalues, color='red')\n",
    "plt.title('RSS: %.4f'%sum((results_MA.fittedvalues - datasetLogDiffShifting)**2))\n",
    "plt.xlim(15000,17500)\n",
    "print('Plotting MA model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cb4538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACF & PACF plots\n",
    "\n",
    "lag_acf = acf(df['logscale+'], nlags=20)\n",
    "lag_pacf = pacf(df['logscale+'], nlags=20, method='ols')\n",
    "\n",
    "#Plot ACF:\n",
    "plt.subplot(121)\n",
    "plt.plot(lag_acf)\n",
    "plt.axhline(y=0, linestyle='--', color='gray')\n",
    "plt.axhline(y=-1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')\n",
    "plt.axhline(y=1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')\n",
    "plt.title('Autocorrelation Function')            \n",
    "\n",
    "#Plot PACF\n",
    "plt.subplot(122)\n",
    "plt.plot(lag_pacf)\n",
    "plt.axhline(y=0, linestyle='--', color='gray')\n",
    "plt.axhline(y=-1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')\n",
    "plt.axhline(y=1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')\n",
    "plt.title('Partial Autocorrelation Function')\n",
    "            \n",
    "plt.tight_layout()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea237f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b0ed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = results_MA.forecast(steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e60880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast future values\n",
    "n_steps = 10  # Number of future steps to forecast\n",
    "forecast = results_MA.forecast(steps=n_steps)\n",
    "\n",
    "# Create a DataFrame for the forecast results\n",
    "forecast_index = pd.date_range(start=df.index[-1], periods=n_steps+1)\n",
    "forecast_df = pd.DataFrame(forecast, index=forecast_index, columns=['forecast'])\n",
    "\n",
    "# Plot the historical data, fitted values, and forecast\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df['logscale+'], 'bo-', label='Historical Data')\n",
    "plt.plot(results_MA.fittedvalues, 'r--', label='Fitted Values')\n",
    "plt.plot(forecast_df, 'go-', label='Forecast')\n",
    "\n",
    "# Plot confidence intervals\n",
    "plt.fill_between(forecast_index, conf_int[:, 0], conf_int[:, 1], color='gray', alpha=0.3)\n",
    "\n",
    "plt.title('ARIMA Model Forecast')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Log Scale')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print the forecast\n",
    "print('Forecasted values:', forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5999ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = results_MA.forecast(steps=1000)\n",
    "\n",
    "plt.plot(df['logscale+'])\n",
    "plt.plot(forecast)\n",
    "plt.xlim(17000,18700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5819ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative = df['total_busyness'].cumsum()\n",
    "cumulative = cumulative.astype('float')\n",
    "\n",
    "rolling_avg = cumulative.rolling(window = 24).mean()\n",
    "rolling_avg.fillna(0, inplace=True)\n",
    "\n",
    "cum_avg = cumulative - rolling_avg\n",
    "possitive_shift = min(cum_avg)\n",
    "possitive_cumulative = cum_avg - possitive_shift +1\n",
    "\n",
    "# plt.plot(logscale)\n",
    "\n",
    "# print(possitive_shift)\n",
    "logscale = np.log(possitive_cumulative)\n",
    "log2 = logscale - logscale.shift()\n",
    "\n",
    "log2.dropna(inplace=True)\n",
    "test_stationarity(log2,24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa80fe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabeb309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['rolling'] = df['total_busyness'].rolling(window = 24).mean()\n",
    "df['rolling'] = df['rolling'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df308bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df['rolling'])\n",
    "plt.xlim(15000,17500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ebf6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform Augmented Dickey–Fuller test:\n",
    "print('Results of Dickey Fuller Test:')\n",
    "dftest = adfuller(df['rolling'], autolag='AIC')\n",
    "\n",
    "dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "for key,value in dftest[4].items():\n",
    "    dfoutput['Critical Value (%s)'%key] = value\n",
    "    \n",
    "print(dfoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fa1f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACF & PACF plots\n",
    "\n",
    "lag_acf = acf(log2, nlags=20)\n",
    "lag_pacf = pacf(log2, nlags=20, method='ols')\n",
    "\n",
    "#Plot ACF:\n",
    "plt.subplot(121)\n",
    "plt.plot(lag_acf)\n",
    "plt.axhline(y=0, linestyle='--', color='gray')\n",
    "plt.axhline(y=-1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')\n",
    "plt.axhline(y=1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')\n",
    "plt.title('Autocorrelation Function')            \n",
    "\n",
    "#Plot PACF\n",
    "plt.subplot(122)\n",
    "plt.plot(lag_pacf)\n",
    "plt.axhline(y=0, linestyle='--', color='gray')\n",
    "plt.axhline(y=-1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')\n",
    "plt.axhline(y=1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')\n",
    "plt.title('Partial Autocorrelation Function')\n",
    "            \n",
    "plt.tight_layout()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4894e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(log2, order=(6,0,0))\n",
    "results_MA = model.fit()\n",
    "plt.plot(log2,'b--')\n",
    "plt.plot(results_MA.fittedvalues, color='red')\n",
    "plt.title('RSS: %.4f'%sum((results_MA.fittedvalues - datasetLogDiffShifting)**2))\n",
    "print('Plotting MA model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a774c423",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = results_MA.forecast(steps=1000)\n",
    "\n",
    "plt.plot(log2)\n",
    "plt.plot(forecast)\n",
    "plt.xlim(17000,18700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c7a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(log2, order=(2,1,2))\n",
    "results_MA = model.fit()\n",
    "\n",
    "plt.plot(log2,'b--')\n",
    "plt.plot(results_MA.fittedvalues, color='red')\n",
    "plt.title('RSS: %.4f'%sum((results_MA.fittedvalues - datasetLogDiffShifting)**2))\n",
    "print('Plotting MA model')\n",
    "# plt.xlim(17000,17500)\n",
    "# plt.ylim(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2e9dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets\n",
    "train_size = int(len(log2) * 0.8)  # 80% for training\n",
    "train_data = df.iloc[:train_size]\n",
    "test_data = df.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b7a204",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_data),len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da3b08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets\n",
    "train_size = int(len(log2) * 0.8)  # 80% for training\n",
    "train_data = log2.iloc[:train_size]\n",
    "test_data = log2.iloc[train_size:]\n",
    "\n",
    "model = ARIMA(train_data, order=(2,1,2))\n",
    "results_MA = model.fit()\n",
    "\n",
    "plt.plot(train_data,'b--')\n",
    "plt.plot(results_MA.fittedvalues, color='red')\n",
    "# plt.title('RSS: %.4f'%sum((results_MA.fittedvalues - datasetLogDiffShifting)**2))\n",
    "print('Plotting MA model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dd1318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assume log2 contains the time series data\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(len(log2) * 0.8)  # 80% for training\n",
    "train_data = df['ts_pos'].iloc[:train_size]\n",
    "test_data = df['ts_pos'].iloc[train_size:]\n",
    "\n",
    "# Train the ARIMA model on the training set\n",
    "model = ARIMA(train_data, order=(2,1,2))\n",
    "results_MA = model.fit()\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = results_MA.forecast(steps=len(test_data))\n",
    "\n",
    "# Evaluate the model using mean squared error\n",
    "mse = mean_squared_error(test_data, predictions)\n",
    "print(f'Mean Squared Error: {mse:.4f}')\n",
    "\n",
    "# Plot the training data, test data, and predictions\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_data, label='Training Data', color='blue')\n",
    "plt.plot(test_data.index, test_data, label='Test Data', color='green')\n",
    "plt.plot(test_data.index, predictions, label='Predictions', color='red')\n",
    "plt.legend()\n",
    "plt.title('ARIMA Model Evaluation')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.xlim(17000,17500)\n",
    "# plt.ylim(-1,1)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f254d99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training data, test data, and predictions\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_data, label='Training Data', color='blue')\n",
    "plt.plot(test_data.index, test_data, label='Test Data', color='green')\n",
    "plt.plot(test_data.index, predictions, label='Predictions', color='red')\n",
    "plt.legend()\n",
    "plt.title('ARIMA Model Evaluation')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.xlim(17000,17500)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de78228d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stationarity(df['ts_pos'],24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16772598",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2328d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cumulative_busyness'] = df['total_busyness'].cumsum()\n",
    "df['moving_average'] = df['cumulative_busyness'].rolling(window = 24)\n",
    "df['ts'] = df['cumulative_busyness'] - df['moving_average']\n",
    "\n",
    "# ts = df['total_busyness'].cumsum() - df['total_busyness'].cumsum().rolling(window = 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b03211",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = df['total_busyness'].astype('float').cumsum() - (df['total_busyness'].astype('float').cumsum()).rolling(window = 24).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786dfcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eb3a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts(dataframe,col):\n",
    "    data = dataframe[col].astype('float')\n",
    "    cumsum = data.cumsum()\n",
    "    moving_avg = cumsum.rolling(window = 24).mean()\n",
    "    ts = cumsum - moving_avg\n",
    "    return ts\n",
    "# df['total_busyness'].cumsum() - (df['total_busyness'].cumsum()).rolling(window = 24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef13d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts(df,col = 'total_busyness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a77a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = df.pipe(ts, col='total_busyness')\n",
    " \n",
    "# calling pipeline\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d903d883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assume log2 contains the time series data\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(len(log2) * 0.8)  # 80% for training\n",
    "train_data = pipeline.iloc[:train_size]\n",
    "test_data = pipeline.iloc[train_size:]\n",
    "\n",
    "# Train the ARIMA model on the training set\n",
    "model = ARIMA(train_data, order=(2,1,2))\n",
    "results_MA = model.fit()\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = results_MA.forecast(steps=len(test_data))\n",
    "\n",
    "# Evaluate the model using mean squared error\n",
    "mse = mean_squared_error(test_data, predictions)\n",
    "print(f'Mean Squared Error: {mse:.4f}')\n",
    "\n",
    "# Plot the training data, test data, and predictions\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_data, label='Training Data', color='blue')\n",
    "plt.plot(test_data.index, test_data, label='Test Data', color='green')\n",
    "plt.plot(test_data.index, predictions, label='Predictions', color='red')\n",
    "plt.legend()\n",
    "plt.title('ARIMA Model Evaluation')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.xlim(17000,17500)\n",
    "# plt.ylim(-1,1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b367585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training data, test data, and predictions\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_data, label='Training Data', color='blue')\n",
    "plt.plot(test_data.index, test_data, label='Test Data', color='green')\n",
    "plt.plot(test_data.index, predictions, label='Predictions', color='red')\n",
    "plt.legend()\n",
    "plt.title('ARIMA Model Evaluation')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.xlim(15500,16000)\n",
    "# plt.ylim(-1,1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de10419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(x):  \n",
    "    query = f'''\n",
    "    WITH min_max_timestamps AS (\n",
    "        SELECT\n",
    "            MIN(TO_TIMESTAMP(timestamp)) AS min_timestamp,\n",
    "            MAX(TO_TIMESTAMP(timestamp)) AS max_timestamp\n",
    "        FROM\n",
    "            busyness_data\n",
    "        WHERE \n",
    "            zonenumber = {x}\n",
    "            AND mode != 'subway'\n",
    "    ),\n",
    "\n",
    "    hour_series AS (\n",
    "        SELECT generate_series(\n",
    "            DATE_TRUNC('hour', min_max_timestamps.min_timestamp),\n",
    "            DATE_TRUNC('hour', min_max_timestamps.max_timestamp),\n",
    "            '1 hour'::interval\n",
    "        ) AS hour\n",
    "        FROM min_max_timestamps\n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "        hour_series.hour AS hour,\n",
    "        COALESCE(SUM(busynessscore), 0) AS total_busyness\n",
    "    FROM\n",
    "        hour_series\n",
    "    LEFT JOIN (\n",
    "        SELECT\n",
    "            DATE_TRUNC('hour', TO_TIMESTAMP(timestamp)) AS hour,\n",
    "            SUM(busynessscore) AS busynessscore\n",
    "        FROM\n",
    "            busyness_data\n",
    "        WHERE \n",
    "            zonenumber = {x}\n",
    "            AND mode = 'taxi'\n",
    "        GROUP BY\n",
    "            DATE_TRUNC('hour', TO_TIMESTAMP(timestamp))\n",
    "    ) AS aggregated_data ON hour_series.hour = aggregated_data.hour\n",
    "    GROUP BY\n",
    "        hour_series.hour\n",
    "    ORDER BY\n",
    "        hour_series.hour;\n",
    "    '''\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4476a2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(x):  \n",
    "    query = f'''\n",
    "    WITH min_max_timestamps AS (\n",
    "        SELECT\n",
    "            MIN(TO_TIMESTAMP(timestamp)) AS min_timestamp,\n",
    "            MAX(TO_TIMESTAMP(timestamp)) AS max_timestamp\n",
    "        FROM\n",
    "            busyness_data_2\n",
    "        WHERE \n",
    "            zonenumber = {x}\n",
    "    ),\n",
    "\n",
    "    hour_series AS (\n",
    "        SELECT generate_series(\n",
    "            DATE_TRUNC('hour', min_max_timestamps.min_timestamp),\n",
    "            DATE_TRUNC('hour', min_max_timestamps.max_timestamp),\n",
    "            '1 hour'::interval\n",
    "        ) AS hour\n",
    "        FROM min_max_timestamps\n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "        hour_series.hour AS hour,\n",
    "        COALESCE(SUM(busynessscore), 0) AS total_busyness\n",
    "    FROM\n",
    "        hour_series\n",
    "    LEFT JOIN (\n",
    "        SELECT\n",
    "            DATE_TRUNC('hour', TO_TIMESTAMP(timestamp)) AS hour,\n",
    "            SUM(busynessscore) AS busynessscore\n",
    "        FROM\n",
    "            busyness_data_2\n",
    "        WHERE \n",
    "            zonenumber = {x}\n",
    "        GROUP BY\n",
    "            DATE_TRUNC('hour', TO_TIMESTAMP(timestamp))\n",
    "    ) AS aggregated_data ON hour_series.hour = aggregated_data.hour\n",
    "    GROUP BY\n",
    "        hour_series.hour\n",
    "    ORDER BY\n",
    "        hour_series.hour;\n",
    "    '''\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ead19a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afc6ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = connect_to_postgres(query(x), postgres_host, postgres_port, database_name, db_user, db_password)\n",
    "\n",
    "# df['unix_timestamp'] = df['ten_minute_interval'].apply(lambda x: x.timestamp()/10**9)\n",
    "# df = df.drop(columns = {'ten_minute_interval'})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea88b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,3):\n",
    "    df = connect_to_postgres(query(i), postgres_host, postgres_port, database_name, db_user, db_password)\n",
    "    pipeline = df.pipe(ts, col='total_busyness')\n",
    "\n",
    "    # calling pipeline\n",
    "    # pipeline\n",
    "\n",
    "    train_size = int(len(log2) * 0.8)  # 80% for training\n",
    "    train_data = pipeline.iloc[:train_size]\n",
    "    test_data = pipeline.iloc[train_size:]\n",
    "\n",
    "    # Train the ARIMA model on the training set\n",
    "    model = ARIMA(train_data, order=(2,1,2))\n",
    "    results_MA = model.fit()\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    predictions = results_MA.forecast(steps=len(test_data))\n",
    "\n",
    "    # Evaluate the model using mean squared error\n",
    "    mse = mean_squared_error(test_data, predictions)\n",
    "    print(f'Mean Squared Error: {mse:.4f}')\n",
    "\n",
    "    # Plot the training data, test data, and predictions\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(train_data, label='Training Data', color='blue')\n",
    "    plt.plot(test_data.index, test_data, label='Test Data', color='green')\n",
    "    plt.plot(test_data.index, predictions, label='Predictions', color='red')\n",
    "    plt.legend()\n",
    "    plt.title(f'ARIMA Model Evaluation plot{i}')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.xlim(17000,17500)\n",
    "#     plt.ylim(-1,1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d9f80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = connect_to_postgres(query(1), postgres_host, postgres_port, database_name, db_user, db_password)\n",
    "\n",
    "# df['unix_timestamp'] = df['ten_minute_interval'].apply(lambda x: x.timestamp()/10**9)\n",
    "# df = df.drop(columns = {'ten_minute_interval'})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9756ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = df.pipe(ts, col='total_busyness')\n",
    "\n",
    "# calling pipeline\n",
    "# pipeline\n",
    "\n",
    "train_size = int(len(pipeline) * 0.8)  # 80% for training\n",
    "train_data = pipeline.iloc[:train_size]\n",
    "test_data = pipeline.iloc[train_size:]\n",
    "\n",
    "# Train the ARIMA model on the training set\n",
    "model = ARIMA(train_data, order=(1,1,2))\n",
    "results_MA = model.fit()\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = results_MA.forecast(steps=len(test_data))\n",
    "\n",
    "# Evaluate the model using mean squared error\n",
    "mse = mean_squared_error(test_data, predictions)\n",
    "print(f'Mean Squared Error: {mse:.4f}')\n",
    "\n",
    "# Plot the training data, test data, and predictions\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(train_data, label='Training Data', color='blue')\n",
    "plt.plot(test_data.index, test_data, label='Test Data', color='green')\n",
    "\n",
    "plt.plot(results_MA.fittedvalues, label='Fit Data', color='red')\n",
    "\n",
    "\n",
    "plt.plot(test_data.index, predictions, label='Predictions', color='red')\n",
    "plt.legend()\n",
    "plt.title(f'ARIMA Model Evaluation plot{i}')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.xlim(17000,17500)\n",
    "#     plt.ylim(-1,1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d54e126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACF & PACF plots\n",
    "\n",
    "lag_acf = acf(df['logshift'], nlags=50)\n",
    "lag_pacf = pacf(df['logshift'], nlags=50, method='ols')\n",
    "\n",
    "#Plot ACF:\n",
    "plt.subplot(121)\n",
    "plt.plot(lag_acf)\n",
    "plt.axhline(y=0, linestyle='--', color='gray')\n",
    "plt.axhline(y=-1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')\n",
    "plt.axhline(y=1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')\n",
    "plt.axhline(y=1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')\n",
    "plt.axvline(6)\n",
    "\n",
    "plt.title('Autocorrelation Function')            \n",
    "\n",
    "#Plot PACF\n",
    "plt.subplot(122)\n",
    "plt.plot(lag_pacf)\n",
    "plt.axhline(y=0, linestyle='--', color='gray')\n",
    "plt.axhline(y=-1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')\n",
    "plt.axvline(2)\n",
    "plt.title('Partial Autocorrelation Function')\n",
    "    \n",
    "plt.tight_layout() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad0434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline[24:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f1693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df['hour'][-200:],df['log'][-200:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9877be8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff15ab2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['positive'] = df['total_busyness'] - min(df['total_busyness']) + 2\n",
    "df['log'] = np.log(df['positive'].astype('float'))\n",
    "rolling_mean = df['log'].rolling(window = 240).mean()\n",
    "print(rolling_mean)\n",
    "df['logshift'] = df['log'] - rolling_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3621ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log'] = np.log(df['positive'].astype('float'))\n",
    "rolling_mean = df['log'].rolling(window = 240).mean()\n",
    "print(rolling_mean)\n",
    "df['logshift'] = df['log'] - rolling_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd92acdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_mean = df['log'].rolling(window = 240).mean()\n",
    "print(rolling_mean)\n",
    "df['logshift'] = df['log'] - rolling_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ca43e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['logshift'] = df['logshift'].fillna(0)\n",
    "# test_stationarity(df['logshift'])\n",
    "test_stationarity(df['logshift'],240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d180f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = df.pipe(ts, col='total_busyness')\n",
    "\n",
    "# calling pipeline\n",
    "# pipeline\n",
    "\n",
    "train_size = int(len(df['logshift']) * 0.8)  # 80% for training\n",
    "train_data = df['logshift'].iloc[:train_size]\n",
    "test_data = df['logshift'].iloc[train_size:]\n",
    "\n",
    "# Train the ARIMA model on the training set\n",
    "model = ARIMA(train_data, order=(2,1,0))\n",
    "results_MA = model.fit()\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = results_MA.forecast(steps=len(test_data))\n",
    "\n",
    "# Evaluate the model using mean squared error\n",
    "mse = mean_squared_error(test_data, predictions)\n",
    "print(f'Mean Squared Error: {mse:.4f}')\n",
    "\n",
    "# Plot the training data, test data, and predictions\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(train_data, label='Training Data', color='blue')\n",
    "plt.plot(test_data.index, test_data, label='Test Data', color='green')\n",
    "\n",
    "plt.plot(results_MA.fittedvalues, label='Fit Data', color='red')\n",
    "\n",
    "\n",
    "plt.plot(test_data.index, predictions, label='Predictions', color='red')\n",
    "plt.legend()\n",
    "plt.title(f'ARIMA Model Evaluation plot{i}')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.xlim(7400,7600)\n",
    "#     plt.ylim(-1,1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed1b5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df['logshift'])\n",
    "plt.xlim(15000,15200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e5887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sktime.forecasting.compose import make_reduction\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sktime.performance_metrics.forecasting import MeanAbsolutePercentageError\n",
    "from sktime.split import temporal_train_test_split\n",
    "\n",
    "y = df['logshift']\n",
    "y_train, y_test = temporal_train_test_split(y)\n",
    "fh = np.arange(1, len(y_test) + 1)  # forecasting horizon\n",
    "regressor = RandomForestRegressor()\n",
    "forecaster = make_reduction(\n",
    "    regressor,\n",
    "    strategy=\"recursive\",\n",
    "    window_length=24,\n",
    ")\n",
    "forecaster.fit(y_train)\n",
    "y_pred = forecaster.predict(fh)\n",
    "smape = MeanAbsolutePercentageError()\n",
    "smape(y_test, y_pred)\n",
    "plt.plot(y_test)\n",
    "plt.plot(y_pred)\n",
    "plt.xlim(17000,17500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e2eb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_test)\n",
    "plt.plot(y_pred)\n",
    "plt.xlim(17000,17500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c15a8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series(df):\n",
    "    minimum = min(df['total_busyness'])\n",
    "    positive = df['total_busyness'] - minimum + 2\n",
    "    log= np.log(positive.astype('float'))\n",
    "    rolling_mean = log.rolling(window = 240).mean()\n",
    "    logshift = log - rolling_mean\n",
    "    logshift = logshift.fillna(0)\n",
    "    \n",
    "    return logshift,rolling_mean, minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43221a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = df.pipe(time_series)\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80783a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,10):\n",
    "    df = connect_to_postgres(query(i), postgres_host, postgres_port, database_name, db_user, db_password)\n",
    "    ts,roll_mean,minimum = df.pipe(time_series)\n",
    "\n",
    "    # calling pipeline\n",
    "    # pipeline\n",
    "\n",
    "\n",
    "    # Train the ARIMA model on the training set\n",
    "    y = ts\n",
    "    y_train, y_test = temporal_train_test_split(y)\n",
    "    fh = np.arange(1, len(y_test) + 1)  # forecasting horizon\n",
    "    regressor = RandomForestRegressor()\n",
    "    forecaster = make_reduction(\n",
    "        regressor,\n",
    "        strategy=\"recursive\",\n",
    "        window_length=24,\n",
    "    )\n",
    "    forecaster.fit(y_train)\n",
    "    y_pred = forecaster.predict(fh)\n",
    "    smape = MeanAbsolutePercentageError()\n",
    "    print('smape:',smape(y_test, y_pred))\n",
    "\n",
    "    # Evaluate the model using mean squared error\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f'Mean Squared Error: {mse:.4f}')\n",
    "\n",
    "    # Plot the training data, test data, and predictions\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_test, label='Test Data')\n",
    "    plt.plot(y_pred, label='Predictions')\n",
    "    plt.legend()\n",
    "    plt.title(f'sktime Model Evaluation plot{i}')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.xlim(17000,17500)\n",
    "#     plt.ylim(-1,1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29535e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = connect_to_postgres(query(1), postgres_host, postgres_port, database_name, db_user, db_password)\n",
    "\n",
    "ts,roll_mean,minimum = df.pipe(time_series)\n",
    "\n",
    "# calling pipeline\n",
    "# pipeline\n",
    "\n",
    "# Train the ARIMA model on the training set\n",
    "y = ts\n",
    "y_train, y_test = temporal_train_test_split(y)\n",
    "fh = np.arange(1, len(y_test) + 1)  # forecasting horizon\n",
    "regressor = RandomForestRegressor()\n",
    "forecaster = make_reduction(\n",
    "    regressor,\n",
    "    strategy=\"recursive\",\n",
    "    window_length=24,\n",
    ")\n",
    "forecaster.fit(y_train)\n",
    "y_pred = forecaster.predict(fh)\n",
    "smape = MeanAbsolutePercentageError()\n",
    "print(smape(y_test, y_pred))\n",
    "\n",
    "# Evaluate the model using mean squared error\n",
    "# print(len(y_test),len(y_pred))\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse:.4f}')\n",
    "\n",
    "# Plot the training data, test data, and predictions\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test, label='Test Data')\n",
    "plt.plot(y_pred, label='Predictions')\n",
    "plt.legend()\n",
    "plt.title(f'sktime Model Evaluation plot{i}')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.xlim(17000,17500)\n",
    "#     plt.ylim(-1,1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e965648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Calculate regression metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the regression metrics\n",
    "print()\n",
    "print(f'Mean Absolute Error (MAE): {mae:.4f}','\\n')\n",
    "print(f'Mean Squared Error (MSE): {mse:.4f}','\\n')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse:.4f}','\\n')\n",
    "print(f'R-squared (R²): {r2:.4f}','\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb14689",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_data),len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db38bdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import MultiPolygon\n",
    "taxi_zone = gpd.read_file('NYCTaxiZones.geojson')\n",
    "taxi_zone.set_geometry(\"geometry\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b9a542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_func(lat,lng, timestamp,taxi_zone):\n",
    "\n",
    "    point = Point(lng, lat)\n",
    "    containing_polygon = taxi_zone['location_id'][taxi_zone['geometry'].contains(point)]\n",
    "    return containing_polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb5fdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lng = 45\n",
    "lat = -37\n",
    "point = Point(lng, lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c5db71",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_func(40.65,-74, 1,taxi_zone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acb7ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cb6b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c99e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts,roll_mean,minimum = df.pipe(time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475d7dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27959f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_list = taxi_zone['location_id'][taxi_zone['borough'] == 'Manhattan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242a2766",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zone_2 = taxi_zone[taxi_zone['location_id'].isin(zone_list)]\n",
    "taxi_zone_2.to_csv('/Users/brianmcmahon/Documents/Research Practicum/Data/taxi_zone.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da038364",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zone['location_id'] = taxi_zone['location_id'].astype('int')\n",
    "taxi_zone.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce0757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_list = [\n",
    "    4, 24, 12, 13, 41, 45, 42, 43, 48, 50, 68, 79, 74, 75, 87, 88, 90, 125, \n",
    "    100, 107, 113, 114, 116, 120, 127, 128, 151, 140, 137, 141, \n",
    "    142, 152, 143, 144, 148, 158, 161, 162, 163, 164, 170, 166, 186, \n",
    "    209, 211, 224, 229, 230, 231, 239, 232, 233, 234, 236, 237, 238, 263, \n",
    "    243, 244, 246, 249, 261, 262\n",
    "]\n",
    "for i in zone_list:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f410b5b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in zone_list:\n",
    "    df = connect_to_postgres(query(i), postgres_host, postgres_port, database_name, db_user, db_password)\n",
    "    ts,roll_mean,minimum = df.pipe(time_series)\n",
    "\n",
    "    # calling pipeline\n",
    "    # pipeline\n",
    "\n",
    "\n",
    "    # Train the ARIMA model on the training set\n",
    "    y = ts\n",
    "    y_train, y_test = temporal_train_test_split(y)\n",
    "    fh = np.arange(1, len(y_test) + 1)  # forecasting horizon\n",
    "    regressor = RandomForestRegressor()\n",
    "    forecaster = make_reduction(\n",
    "        regressor,\n",
    "        strategy=\"recursive\",\n",
    "        window_length=24,\n",
    "    )\n",
    "    forecaster.fit(y_train)\n",
    "    y_pred = forecaster.predict(fh)\n",
    "    smape = MeanAbsolutePercentageError()\n",
    "    print('smape:',smape(y_test, y_pred))\n",
    "\n",
    "    # Evaluate the model using mean squared error\n",
    "    # Calculate regression metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Print the regression metrics\n",
    "    print()\n",
    "    print(f'Mean Absolute Error (MAE): {mae:.4f}','\\n')\n",
    "    print(f'Mean Squared Error (MSE): {mse:.4f}','\\n')\n",
    "    print(f'Root Mean Squared Error (RMSE): {rmse:.4f}','\\n')\n",
    "    print(f'R-squared (R²): {r2:.4f}','\\n')\n",
    "\n",
    "    # Plot the training data, test data, and predictions\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_test, label='Test Data')\n",
    "    plt.plot(y_pred, label='Predictions')\n",
    "    plt.legend()\n",
    "    plt.title(f'sktime Model Evaluation plot{i}')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.xlim(17000,17500)\n",
    "#     plt.ylim(-1,1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a229a838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def busyness_force(lat,lng,tazi_zone,list_of_zones):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e739c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(lat_1,lng_1,lat_2,lng_2):\n",
    "    '''\n",
    "    Returns the radial distance between two points\n",
    "    '''\n",
    "    x = lng_1-lng_2\n",
    "    y = lat_1-lat_2\n",
    "    h = np.sqrt(x**2 + y**2)\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47152b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c92a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zone['centroid'] = taxi_zone['geometry'].centroid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4639e065",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b43340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = connect_to_postgres(query(262), postgres_host, postgres_port, database_name, db_user, db_password)\n",
    "\n",
    "ts,roll_mean,minimum = df.pipe(time_series)\n",
    "\n",
    "# calling pipeline\n",
    "# pipeline\n",
    "\n",
    "# Train the ARIMA model on the training set\n",
    "y = ts\n",
    "y_train, y_test = temporal_train_test_split(y)\n",
    "# fh = np.arange(1, len(y_test) + 1)  # forecasting horizon\n",
    "fh = np.arange(1, 2160)  # forecasting horizon 3 months\n",
    "\n",
    "regressor = RandomForestRegressor()\n",
    "forecaster = make_reduction(\n",
    "    regressor,\n",
    "    strategy=\"recursive\",\n",
    "    window_length=168,\n",
    ")\n",
    "forecaster.fit(y_train)\n",
    "y_pred = forecaster.predict(fh)\n",
    "# smape = MeanAbsolutePercentageError()\n",
    "# print(smape(y_test, y_pred))\n",
    "\n",
    "# # Evaluate the model using mean squared error\n",
    "# # print(len(y_test),len(y_pred))\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# print(f'Mean Squared Error: {mse:.4f}')\n",
    "\n",
    "# Plot the training data, test data, and predictions\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test, label='Test Data')\n",
    "plt.plot(y_pred, label='Predictions')\n",
    "plt.legend()\n",
    "plt.title(f'sktime Model Evaluation plot{12}')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "# plt.xlim(17000,17500)\n",
    "#     plt.ylim(-1,1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f82f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_train)\n",
    "plt.plot(y_test, label='Test Data')\n",
    "plt.plot(y_pred, label='Predictions')\n",
    "plt.legend()\n",
    "plt.title(f'sktime Model Evaluation plot{12}')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "# plt.xlim(13500,14000)\n",
    "\n",
    "# plt.xlim(16000,16500)\n",
    "# plt.ylim(-1,1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07520fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad406e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_host = 'localhost'\n",
    "postgres_port = 5432 \n",
    "database_name = 'busyness'\n",
    "db_user = 'crafty'\n",
    "db_password = 'winner'\n",
    "table_name = 'public.busyness_data_2'\n",
    "df = connect_to_postgres(query(42), postgres_host, postgres_port, database_name, db_user, db_password)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0ebdb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in zone_list:\n",
    "    df = connect_to_postgres(query(i), postgres_host, postgres_port, database_name, db_user, db_password)\n",
    "    mean_value = df['total_busyness'].mean().mean()\n",
    "    ts,roll_mean,minimum = df.pipe(time_series)\n",
    "    \n",
    "    y = ts\n",
    "    y_train, y_test = temporal_train_test_split(y)\n",
    "#     fh = np.arange(1, len(y_test) + 1)  # forecasting horizon\n",
    "    fh = np.arange(1, 2160)  # forecasting horizon 3 months\n",
    "\n",
    "    regressor = RandomForestRegressor()\n",
    "    forecaster = make_reduction(\n",
    "        regressor,\n",
    "        strategy=\"recursive\",\n",
    "        window_length=168,\n",
    "    )\n",
    "    forecaster.fit(y_train)\n",
    "    y_pred = forecaster.predict(fh)\n",
    "    smape = MeanAbsolutePercentageError()\n",
    "    \n",
    "    with open(f'/Users/brianmcmahon/Documents/Research Practicum/Models/forecaster_model_zone_{i}.pkl', 'wb') as file:\n",
    "        pickle.dump(forecaster, file)\n",
    "    \n",
    "    \n",
    "    print('smape:',smape(y_test, y_pred[:len(y_test)]))\n",
    "    smape_2 = smape(y_test, y_pred[:len(y_test)])\n",
    "    # Evaluate the model using mean squared error\n",
    "    # Calculate regression metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred[:len(y_test)])\n",
    "    mse = mean_squared_error(y_test, y_pred[:len(y_test)])\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred[:len(y_test)])\n",
    "\n",
    "    # Print the regression metrics\n",
    "    print()\n",
    "    print(f'Mean Absolute Error (MAE): {mae:.4f}','\\n')\n",
    "    print(f'Mean Squared Error (MSE): {mse:.4f}','\\n')\n",
    "    print(f'Root Mean Squared Error (RMSE): {rmse:.4f}','\\n')\n",
    "    print(f'R-squared (R²): {r2:.4f}','\\n')\n",
    "    \n",
    "    stats_file_path = f'/Users/brianmcmahon/Documents/Research Practicum/Models/model_statistics_zone_{i}.txt'\n",
    "    write_model_statistics(stats_file_path, i, smape_2, mae, mse, rmse, r2, roll_mean, minimum, mean_value)\n",
    "    \n",
    "    start_hour = max(df['hour'])  # specify the start hour\n",
    "    zone = i  # specify the zone\n",
    "    x = np.exp(y_pred)*mean_value\n",
    "    new_df = create_dataframe(start_hour, zone, x)\n",
    "    \n",
    "    postgres_host = 'localhost'\n",
    "    postgres_port = 5432 \n",
    "    database_name = 'busyness'\n",
    "    db_user = 'crafty'\n",
    "    db_password = 'winner'\n",
    "    table_name = 'public.busyness_score'\n",
    "\n",
    "    upload_to_postgres(new_df, postgres_host, postgres_port, database_name, db_user, db_password, table_name)\n",
    "    \n",
    "    # Plot the training data, test data, and predictions\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_test, label='Test Data')\n",
    "    plt.plot(y_pred, label='Predictions')\n",
    "    plt.legend()\n",
    "    plt.title(f'sktime Model Evaluation plot{i}')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "#     plt.xlim(17000,17500)\n",
    "#     plt.ylim(-1,1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a560de4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c49813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a file\n",
    "with open('/Users/brianmcmahon/Documents/Research Practicum/Models/forecaster_model.pkl', 'wb') as file:\n",
    "    pickle.dump(forecaster, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a11040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_model_statistics(file_path, zone, smape, mae, mse, rmse, r2, roll_mean, minimum,mean_value):\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(f\"Model Statistics for Zone: {zone}\\n\")\n",
    "        f.write(f\"sMAPE: {smape:.4f}\\n\")\n",
    "        f.write(f\"Mean Absolute Error (MAE): {mae:.4f}\\n\")\n",
    "        f.write(f\"Mean Squared Error (MSE): {mse:.4f}\\n\")\n",
    "        f.write(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\\n\")\n",
    "        f.write(f\"R-squared (R²): {r2:.4f}\\n\")\n",
    "#         f.write(f\"Rolling Mean: {roll_mean}\\n\")\n",
    "        f.write(f\"Minimum: {minimum}\\n\")\n",
    "        f.write(f\"Mean Value of DataFrame: {mean_value:.4f}\\n\")\n",
    "    print(f\"Statistics written to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e857533",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['total_busyness'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7f486a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a3fa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Function to compute the mean of a DataFrame and update the statistics file\n",
    "def update_statistics_with_mean(stats_file_path, df):\n",
    "    # Compute the mean value of the DataFrame\n",
    "    mean_value = df['total_busyness'].mean().mean()\n",
    "\n",
    "    # Read the existing statistics text file\n",
    "    with open(stats_file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Add the mean value to the statistics\n",
    "    lines.append(f\"Mean Value of DataFrame: {mean_value:.4f}\\n\")\n",
    "\n",
    "    # Write the updated statistics back to the text file\n",
    "    with open(stats_file_path, 'w') as file:\n",
    "        file.writelines(lines)\n",
    "    \n",
    "    print(f\"Updated {stats_file_path} with mean value.\")\n",
    "\n",
    "# Directory where the statistics files are stored\n",
    "stats_directory = '/Users/brianmcmahon/Documents/Research Practicum/Models'\n",
    "\n",
    "# Iterate over each zone and update the corresponding statistics file\n",
    "for i in zone_list:\n",
    "    # Load the DataFrame for the current zone\n",
    "    df = connect_to_postgres(query(i), postgres_host, postgres_port, database_name, db_user, db_password)\n",
    "\n",
    "    # Path to the statistics file\n",
    "    stats_file_path = os.path.join(stats_directory, f'model_statistics_zone_{i}.txt')\n",
    "\n",
    "    # Update the statistics file with the mean value of the DataFrame\n",
    "    update_statistics_with_mean(stats_file_path, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcebc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc3309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.exp(y_pred)*14.309122203098108)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a76d812",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(df['hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3de8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(df['hour'])+datetime.month(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c70f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1063e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_until = max(df['hour']) + relativedelta(months=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0456961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fh = np.arange(1, 2160)  # forecasting horizon 3 months\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ab1e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "3*30*24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c467b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.exp(y_pred)*14.309122203098108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c80e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5182228d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(start_hour, zone,x):\n",
    "    # Starting datetime\n",
    "#     start_time = datetime.strptime(start_hour, \"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Create a list of hours iterating one hour for each row\n",
    "    hours = [start_hour + timedelta(hours=i) for i in range(len(x))]\n",
    "    \n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'hour': hours,\n",
    "        'busy_score': x,\n",
    "        'zone': zone\n",
    "    })\n",
    "    df['hour'] = df['hour'].dt.tz_localize(None)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5486d3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "start_hour = max(df['hour'])  # specify the start hour\n",
    "zone = 1  # specify the zone\n",
    "\n",
    "new_df = create_dataframe(start_hour, zone, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd5225a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873ac014",
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2592ab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be5738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import psycopg2\n",
    "from io import StringIO\n",
    "\n",
    "def upload_to_postgres(dataframe, postgres_host, postgres_port, database_name, db_user, db_password, table_name):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    conn = psycopg2.connect(\n",
    "        database=database_name,\n",
    "        user=db_user,\n",
    "        password=db_password,\n",
    "        host=postgres_host,\n",
    "        port=postgres_port\n",
    "    )\n",
    "    print(\"PostgreSQL connection opened\")\n",
    "\n",
    "    # Example: Execute SQL query\n",
    "    cursor = conn.cursor()\n",
    "        \n",
    "    # Create a temporary table with DOUBLE PRECISION for lat/lng and BIGINT for Unix timestamps\n",
    "    cursor.execute('''\n",
    "    CREATE TEMP TABLE temp (\n",
    "        hour TIMESTAMP WITHOUT TIME ZONE,\n",
    "        score DOUBLE PRECISION,\n",
    "        zone INTEGER\n",
    "    );\n",
    "    ''')\n",
    "\n",
    "    # Using StringIO to perform bulk insert into the temporary table\n",
    "    buffer = StringIO()\n",
    "    dataframe.to_csv(buffer, index=False, header=False)\n",
    "    buffer.seek(0)\n",
    "\n",
    "    cursor.copy_expert(f\"\"\"\n",
    "    COPY temp (hour, score, zone)\n",
    "    FROM STDIN WITH CSV\n",
    "    \"\"\", buffer)\n",
    "\n",
    "    # Insert data from the temporary table to the target table with conflict handling\n",
    "    cursor.execute(f\"\"\"\n",
    "    INSERT INTO {table_name} (hour, score, zone)\n",
    "    SELECT hour, score, zone\n",
    "    FROM temp\n",
    "    ON CONFLICT DO NOTHING;\n",
    "    \"\"\")\n",
    "\n",
    "    # Commit the transaction\n",
    "    try:\n",
    "        conn.commit()\n",
    "        print(\"Data inserted successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "    # Close the PostgreSQL connection\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    print(\"PostgreSQL connection closed\")\n",
    "\n",
    "    print(f'connect_to_postgres took {time.time() - start_time} seconds to complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99381c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_host = 'localhost'\n",
    "postgres_port = 5432 \n",
    "database_name = 'busyness'\n",
    "db_user = 'crafty'\n",
    "db_password = 'winner'\n",
    "table_name = 'public.busyness_score'\n",
    "\n",
    "connect_to_postgres(new_df, postgres_host, postgres_port, database_name, db_user, db_password, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd15aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "connect_to_postgres(new_df, postgres_host, postgres_port, database_name, db_user, db_password)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936b6c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "    SELECT\n",
    "    *\n",
    "    FROM\n",
    "        busyness_score; \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1c3c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = connect_to_postgres(query, postgres_host, postgres_port, database_name, db_user, db_password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316ab5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca100db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('/Users/brianmcmahon/Downloads/MHTN_zoned_streets.csv')\n",
    "df2 = df2.drop(columns = ['address','street_geometry','zone_name','zone_geometry','zone_id'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0917bc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.drop(columns = ['address','street_geometry','zone_name','zone_geometry','zone_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505798da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab44ba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zone = gpd.read_file('NYCTaxiZones.geojson')\n",
    "taxi_zone.set_geometry(\"geometry\", inplace=True)\n",
    "taxi_zone['centroid'] = taxi_zone['geometry'].centroid\n",
    "taxi_zone['location_id'] = taxi_zone['location_id'].astype('int')\n",
    "\n",
    "taxi_zone = taxi_zone.drop(columns = ['shape_area','objectid','shape_leng','borough','geometry','zone'])\n",
    "taxi_zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572e5905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_rows(df, target_datetime):\n",
    "    \n",
    "#     df = df.merge(taxi_zone, left_on='zone', right_on='location_id', how='left')\n",
    "#     df = df.drop(columns = ['location_id'])\n",
    "\n",
    "\n",
    "    # Ensure 'hour' is datetime\n",
    "    df['hour'] = pd.to_datetime(df['hour'])\n",
    "    \n",
    "    # Calculate the absolute difference between each 'hour' and the target_datetime\n",
    "    df['time_diff'] = abs(df['hour'] - target_datetime)\n",
    "    \n",
    "    # Sort by zone and then by the time difference\n",
    "    df_sorted = df.sort_values(by=['zone', 'time_diff'])\n",
    "    \n",
    "    # Drop duplicates to keep the row with the smallest time difference for each zone\n",
    "    closest_rows = df_sorted.drop_duplicates(subset=['zone'], keep='first')\n",
    "    \n",
    "    # Drop the temporary 'time_diff' column\n",
    "    closest_rows = closest_rows.drop(columns=['time_diff'])\n",
    "    \n",
    "    return closest_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f3b50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c80b1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_datetime = pd.to_datetime('2024-06-04 02:30:00')\n",
    "\n",
    "closest_rows = find_closest_rows(df, target_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d87523",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "closest_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766341bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['score'] = sum(closest_rows['score'])*np.array([i for i in range(len(df2['score']))])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4e139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i for i in range(len(closest_rows['score']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db886343",
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_rows['score']*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4841d763",
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_rows['centroid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2c35f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(geometry, closest_rows):\n",
    "    x = [(geometry.distance(i))**2 for i in closest_rows['centroid']]\n",
    "    y = closest_rows['score']\n",
    "    \n",
    "    return sum(y/x)\n",
    "#     return geometry.distance(reference_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643808d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import wkt\n",
    "\n",
    "df2['street_centroid'] = df2['street_centroid'].apply(wkt.loads)\n",
    "\n",
    "gdf = gpd.GeoDataFrame(df2, geometry='street_centroid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5908bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf['distance_to_ref'] = \n",
    "gdf['estimate'] = gdf['street_centroid'].apply(calculate_distance, closest_rows = closest_rows)/10**5\n",
    "gdf['estimate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4080ba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968accd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import wkb\n",
    "\n",
    "df = connect_to_postgres(query, postgres_host, postgres_port, database_name, db_user, db_password)\n",
    "# print(df)\n",
    "df['centroid'] = df['centroid'].apply(wkb.loads)\n",
    "df\n",
    "# gdf = gpd.GeoDataFrame(df, geometry='centroid', crs = 4326)\n",
    "# gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4b61c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('/Users/brianmcmahon/Downloads/MHTN_zoned_streets.csv')\n",
    "df2 = df2.drop(columns = ['address','street_geometry','zone_name','zone_geometry','zone_id'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e20349",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_datetime = pd.to_datetime('2024-06-04 02:30:00')\n",
    "\n",
    "taxi_zone = gpd.read_file('NYCTaxiZones.geojson')\n",
    "taxi_zone.set_geometry(\"geometry\", inplace=True)\n",
    "taxi_zone['centroid'] = taxi_zone['geometry'].centroid\n",
    "taxi_zone['location_id'] = taxi_zone['location_id'].astype('int')\n",
    "\n",
    "taxi_zone = taxi_zone.drop(columns = ['shape_area','objectid','shape_leng','borough','geometry','zone'])\n",
    "taxi_zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26cc582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_busyness(query, target_datetime, postgres_host, postgres_port, database_name, db_user, db_password):\n",
    "    start_time = time.time()\n",
    "\n",
    "    df = connect_to_postgres(query, postgres_host, postgres_port, database_name, db_user, db_password)\n",
    "    df['centroid'] = df['centroid'].apply(wkb.loads)\n",
    "\n",
    "    closest_rows = find_closest_rows(df, target_datetime)\n",
    "    \n",
    "    df2 = pd.read_csv('/Users/brianmcmahon/Downloads/MHTN_zoned_streets.csv')\n",
    "    df2 = df2.drop(columns = ['address','zone_name','zone_geometry','zone_id'])#,'street_geometry'\n",
    "    df2['street_centroid'] = df2['street_centroid'].apply(wkt.loads)\n",
    "    gdf = gpd.GeoDataFrame(df2, geometry='street_centroid')\n",
    "    gdf['estimate'] = gdf['street_centroid'].apply(calculate_distance, closest_rows = closest_rows)/10**5\n",
    "    print(time.time() - start_time)\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af5a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_datetime = pd.to_datetime('2024-06-10 12:30:00')\n",
    "\n",
    "estimate = estimate_busyness(query, target_datetime, postgres_host, postgres_port, database_name, db_user, db_password)\n",
    "estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5e65f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2278d19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import psycopg2\n",
    "from io import StringIO\n",
    "from shapely import wkb,wkt\n",
    "\n",
    "def upload_to_postgres(dataframe, postgres_host, postgres_port, database_name, db_user, db_password, table_name):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    conn = psycopg2.connect(\n",
    "        database=database_name,\n",
    "        user=db_user,\n",
    "        password=db_password,\n",
    "        host=postgres_host,\n",
    "        port=postgres_port\n",
    "    )\n",
    "    print(\"PostgreSQL connection opened\")\n",
    "\n",
    "    # Example: Execute SQL query\n",
    "    cursor = conn.cursor()\n",
    "        \n",
    "    # Create a temporary table with DOUBLE PRECISION for lat/lng and BIGINT for Unix timestamps\n",
    "    cursor.execute('''\n",
    "    CREATE TEMP TABLE temp (\n",
    "        zone INTEGER,\n",
    "        centroid GEOMETRY\n",
    "    );\n",
    "    ''')\n",
    "\n",
    "    # Using StringIO to perform bulk insert into the temporary table\n",
    "    buffer = StringIO()\n",
    "    dataframe.to_csv(buffer, index=False, header=False)\n",
    "    buffer.seek(0)\n",
    "\n",
    "    cursor.copy_expert(f\"\"\"\n",
    "    COPY temp (zone, centroid)\n",
    "    FROM STDIN WITH CSV\n",
    "    \"\"\", buffer)\n",
    "\n",
    "    # Insert data from the temporary table to the target table with conflict handling\n",
    "    cursor.execute(f\"\"\"\n",
    "    INSERT INTO {table_name} (zone, centroid)\n",
    "    SELECT zone, centroid\n",
    "    FROM temp\n",
    "    ON CONFLICT DO NOTHING;\n",
    "    \"\"\")\n",
    "\n",
    "    # Commit the transaction\n",
    "    try:\n",
    "        conn.commit()\n",
    "        print(\"Data inserted successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "    # Close the PostgreSQL connection\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    print(\"PostgreSQL connection closed\")\n",
    "\n",
    "    print(f'connect_to_postgres took {time.time() - start_time} seconds to complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236641f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload_to_postgres(taxi_zone, postgres_host, postgres_port, database_name, db_user, db_password, 'taxi_zone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33560be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import psycopg2\n",
    "from io import StringIO\n",
    "from shapely import wkb,wkt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import geopandas as gpd\n",
    "import os\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import MultiPolygon, Polygon\n",
    "\n",
    "\n",
    "postgres_host = 'localhost'\n",
    "postgres_port = 5432 \n",
    "database_name = 'busyness'\n",
    "db_user = 'crafty'\n",
    "db_password = 'winner'\n",
    "table_name = 'public.busyness_data'\n",
    "\n",
    "query = '''\n",
    "    SELECT\n",
    "    *\n",
    "    FROM\n",
    "        busyness_score; \n",
    "    '''\n",
    "\n",
    "def connect_to_postgres(query, postgres_host, postgres_port, database_name, db_user , db_password):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    conn = None\n",
    "    try:\n",
    "        # Connect to PostgreSQL\n",
    "        conn = psycopg2.connect(\n",
    "            database=database_name,\n",
    "            user=db_user,\n",
    "            password=db_password,\n",
    "            host=postgres_host,\n",
    "            port=postgres_port\n",
    "        )\n",
    "        print(\"PostgreSQL connection opened\")\n",
    "        \n",
    "        # Create a cursor object\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Execute the query\n",
    "        cursor.execute(query)\n",
    "        print(\"Query executed successfully\")\n",
    "        \n",
    "        # Fetch all rows from the executed query\n",
    "        rows = cursor.fetchall()\n",
    "        \n",
    "        # Get column names from the cursor\n",
    "        colnames = [desc[0] for desc in cursor.description]\n",
    "        \n",
    "        # Convert the result to a pandas DataFrame\n",
    "        df = pd.DataFrame(rows, columns=colnames)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return None\n",
    "\n",
    "    finally:\n",
    "        # Closing the connection\n",
    "        if conn:\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            print(\"PostgreSQL connection closed.\")\n",
    "\n",
    "    print(f'connect_to_postgres took {time.time() - start_time} seconds to complete')\n",
    "    \n",
    "def find_closest_rows(df, target_datetime):\n",
    "    \n",
    "#     df = df.merge(taxi_zone, left_on='zone', right_on='location_id', how='left')\n",
    "#     df = df.drop(columns = ['location_id'])\n",
    "\n",
    "\n",
    "    # Ensure 'hour' is datetime\n",
    "    df['hour'] = pd.to_datetime(df['hour'])\n",
    "    \n",
    "    # Calculate the absolute difference between each 'hour' and the target_datetime\n",
    "    df['time_diff'] = abs(df['hour'] - target_datetime)\n",
    "    \n",
    "    # Sort by zone and then by the time difference\n",
    "    df_sorted = df.sort_values(by=['zone', 'time_diff'])\n",
    "    \n",
    "    # Drop duplicates to keep the row with the smallest time difference for each zone\n",
    "    closest_rows = df_sorted.drop_duplicates(subset=['zone'], keep='first')\n",
    "    \n",
    "    # Drop the temporary 'time_diff' column\n",
    "    closest_rows = closest_rows.drop(columns=['time_diff'])\n",
    "    \n",
    "    return closest_rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe004a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "    SELECT\n",
    "    *\n",
    "    FROM\n",
    "        busyness_score; \n",
    "    '''\n",
    "postgres_host = 'localhost'\n",
    "postgres_port = 5432 \n",
    "database_name = 'busyness'\n",
    "db_user = 'crafty'\n",
    "db_password = 'winner'\n",
    "table_name = 'public.busyness_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bf6e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_datetime = pd.to_datetime('2024-06-05 12:30:00')\n",
    "\n",
    "estimate = estimate_busyness(query, target_datetime, postgres_host, postgres_port, database_name, db_user, db_password)\n",
    "estimate_2 = estimate.sort_values(by='estimate', ascending=False)\n",
    "estimate_2\n",
    "e3 = estimate_2.head(60)\n",
    "e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db775a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_datetime = pd.to_datetime('2024-06-04 02:30:00')\n",
    "\n",
    "estimate = estimate_busyness(query, target_datetime, postgres_host, postgres_port, database_name, db_user, db_password)\n",
    "estimate_2 = estimate.sort_values(by='estimate', ascending=False).head(10)\n",
    "e3 = estimate_2.head(10)\n",
    "e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995f8c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "\n",
    "def parse_event_datetime(dt_str):\n",
    "    formats = [\"%Y-%m-%dT%H:%M:%S.%f\", \"%Y-%m-%d %H:%M:%S\", \"%Y-%m-%dT%H:%M:%S\"]\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            return pd.to_datetime(dt_str, format=fmt)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    raise ValueError(f\"Unable to parse datetime string: {dt_str}\")\n",
    "\n",
    "def find_nearest_event(events_df, location_lat, location_lng, user_datetime):\n",
    "    events_df['Distance'] = events_df.apply(\n",
    "        lambda row: geodesic((location_lat, location_lng), (row['Latitude'], row['Longitude'])).meters,\n",
    "        axis=1\n",
    "    )\n",
    "    valid_events = events_df[\n",
    "        (events_df['Start Date/Time'] <= user_datetime) & (events_df['End Date/Time'] >= user_datetime)]\n",
    "    if valid_events.empty:\n",
    "        return None\n",
    "    return valid_events.loc[valid_events['Distance'].idxmin()]\n",
    "\n",
    "def get_recommendations(busy_score_data, events_df, user_zone, user_datetime):\n",
    "    events_df['Start Date/Time'] = pd.to_datetime(events_df['Start Date/Time'],format='mixed')\n",
    "    events_df['End Date/Time'] = pd.to_datetime(events_df['End Date/Time'],format='mixed')\n",
    "    relevant_areas = busy_score_data\n",
    "\n",
    "    if relevant_areas.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    recommendations = relevant_areas.apply(\n",
    "        lambda row: pd.Series({\n",
    "            'Latitude': row['street_centroid'].y,\n",
    "            'Longitude': row['street_centroid'].x,\n",
    "            'Zone': user_zone,\n",
    "            'Score': row['estimate'],\n",
    "            'DistanceToEvent': find_nearest_event(events_df, row['street_centroid'].y, row['street_centroid'].x, user_datetime)['Distance'] if find_nearest_event(events_df, row['street_centroid'].y, row['street_centroid'].x, user_datetime) is not None else float('inf')\n",
    "        }),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    recommendations = recommendations.sort_values(by='Score', ascending=False)\n",
    "\n",
    "    return recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3682472",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df['Start Date/Time'] = pd.to_datetime(events_df['Start Date/Time'],format='mixed')\n",
    "events_df['End Date/Time'] = pd.to_datetime(events_df['End Date/Time'],format='mixed')\n",
    "events_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1b5abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_datetime = pd.to_datetime('2024-07-27 17:30:00')\n",
    "\n",
    "get_recommendations(e3, events_df, 3, target_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc5f02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df = pd.read_csv('combined_nyc_events.csv')\n",
    "find_nearest_event(events_df, events_df, 3, target_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21333a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df['Start Date/Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b231c3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'output.csv' with the actual path returned by connect_to_postgres\n",
    "csv_path = '/Users/brianmcmahon/Documents/Research Practicum/Data/test.csv'\n",
    "\n",
    "# Load the CSV into a pandas DataFrame\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Now you can use df for further analysis or processing in your Jupyter Notebook\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cd02d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['centroid'].str[10:]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
